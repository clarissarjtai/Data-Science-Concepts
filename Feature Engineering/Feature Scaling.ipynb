{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is **Feature Scaling**?\n",
    "Feature scaling is a method used to normalize/scale the range of independent variables or features of data. [2]\n",
    "The two most discussed scaling methods are Normalization and Standardization. [1]\n",
    "+ Normalization typically means rescales the values into a range of [0,1]. \n",
    "+ Standardization typically means rescales data to have a mean of 0 and a standard deviation of 1 (unit variance). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why **Feature Scaling**?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Scaling Methods\n",
    "1. Standard Scaling\n",
    "+ Explanation: Substract data points with mean and divided by the variance.\n",
    "+ Main Effect: Removes the effect of mean and scales the data to unit variance. \n",
    "+ The scaling shrinks the range of the feature values. The maximum range of the scaled data is **uncertain**.\n",
    "+ Influence by Outliers: However, the outliers have an influence when computing the empirical mean and standard deviation. Note in particular that because the outliers on each feature have different magnitudes, the spread of the transformed data on each feature could be very different. StandardScaler therefore **cannot guarantee balanced feature scales in the presence of outliers**.\n",
    "\n",
    "2. Min-max Scaling\n",
    "+ Explanation: Substract data points with minimun values and divided by the range.\n",
    "+ Main Effect: It is the simplest method and consists in rescaling the range of features to scale the range in [0, 1] or [−1, 1]\n",
    "+ The maximum range of the scaled data is **certain**.\n",
    "+ Influence by Outliers: Very sensitive to the presence of outliers. For example, when the range is large, the scaled data could shrink a lot more than expectation.\n",
    "\n",
    "3. Max-abs Scaling\n",
    "+ Explanation: Divide each data points with its maximun absolute data.\n",
    "+ Main Effect: This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.\n",
    "+ Similar to min-max scaling. The maximum range of the scaled data is **certain**.\n",
    "+ Influence by Outliers: Very sensitive to the presence of outliers. For example, when the range is large, the scaled data could shrink a lot more than expectation.\n",
    "\n",
    "4. Robust Scaling\n",
    "+ Explanation: Substract data with the median and scales the data according to the **quantile range** (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n",
    "+ Main Effect: This normalization is based on the fact that for a normal distribution, the interquartile range is approximately 1.349 times the standard deviation. The mean of the scaled data will not be zero, and the information of variance will also be preserved.\n",
    "+ The maximum range of the scaled data is **uncertain**.\n",
    "+ Influence by Outliers: The interquartile range is less effected by extremes than the standard deviation. The effect of outliers will not impact the range of the scaled data.\n",
    "\n",
    "5. Power Transformation\n",
    "+ Explanation: In statistics, a power transform is a family of functions applied to create a monotonic transformation of data using power functions. It is a data transformation technique used to stabilize variance, make the data more normal distribution-like, improve the validity of measures of association (such as the Pearson correlation between variables), and for other data stabilization procedures.\n",
    "+ Main Effect: Applies a power transformation to each feature to make the data more Gaussian-like in order to stabilize variance and minimize skewness.\n",
    "+ Yeo–Johnson transformation: The Yeo–Johnson transformation[15] allows also for zero and negative values of {\\displaystyle y}y. {\\displaystyle \\lambda }\\lambda  can be any real number, where {\\displaystyle \\lambda =1}\\lambda =1 produces the identity transformation. \n",
    "+ Box–Cox transformation: \n",
    "+ Currently the Yeo-Johnson and Box-Cox transforms are supported in **scikit-learn** and the optimal scaling factor is determined via maximum likelihood estimation in both methods. By default, PowerTransformer applies zero-mean, unit variance normalization. Note that Box-Cox can only be applied to strictly positive data, if negative values are present the Yeo-Johnson transformed is preferred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. [Normalization vs Standardization — Quantitative analysis](https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf)\n",
    "2. [Wikipedia - Feature Scaling](https://en.wikipedia.org/wiki/Feature_scaling)\n",
    "3. [Compare the effect of different scalers on data with outliers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)\n",
    "4. [Wikipedia - Power Transform](https://en.wikipedia.org/wiki/Power_transform)\n",
    "5. []()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
